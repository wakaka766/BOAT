<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>boat_torch.utils &mdash; BOAT 0.0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=92fd9be5" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="_static/custom.css?v=751030aa" />

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="_static/jquery.js?v=5d32c60e"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="_static/documentation_options.js?v=d45e8c67"></script>
        <script src="_static/doctools.js?v=9a2dae69"></script>
        <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Data HyperCleaning" href="data_hyper_cleaning_example.html" />
    <link rel="prev" title="boat_torch.hyper_ol" href="boat_torch.hyper_ol.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            BOAT
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Installation Guide:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="description.html">BOAT Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="install_guide.html">Installation and Usage Guide</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="boat_torch.html">BOAT Structure</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="boat_torch.html#module-boat_torch.boat_opt">Core Problem Class</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="boat_torch.html#main-subpackages">Main Subpackages</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="boat_torch.dynamic_ol.html">boat_torch.dynamic_ol</a></li>
<li class="toctree-l3"><a class="reference internal" href="boat_torch.fogm.html">boat_torch.fogm</a></li>
<li class="toctree-l3"><a class="reference internal" href="boat_torch.hyper_ol.html">boat_torch.hyper_ol</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">boat_torch.utils</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#submodules">Submodules</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-boat_torch.utils.op_utils">boat_torch.utils.op_utils</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="boat_torch.html#module-boat_torch.operation_registry">Extension with Operation_Registry</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Example:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="data_hyper_cleaning_example.html">Data HyperCleaning</a></li>
<li class="toctree-l1"><a class="reference internal" href="l2_regularization_example.html">L2 Regularization</a></li>
<li class="toctree-l1"><a class="reference internal" href="meta_learning_example.html">Meta-Learning</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">BOAT</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="boat_torch.html">BOAT Structure</a></li>
      <li class="breadcrumb-item active">boat_torch.utils</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/boat_torch.utils.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="boat-torch-utils">
<h1>boat_torch.utils<a class="headerlink" href="#boat-torch-utils" title="Link to this heading"></a></h1>
<section id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Link to this heading"></a></h2>
</section>
<section id="module-boat_torch.utils.op_utils">
<span id="boat-torch-utils-op-utils"></span><h2>boat_torch.utils.op_utils<a class="headerlink" href="#module-boat_torch.utils.op_utils" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="boat_torch.utils.op_utils.DynamicalSystemRules">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">boat_torch.utils.op_utils.</span></span><span class="sig-name descname"><span class="pre">DynamicalSystemRules</span></span><a class="reference internal" href="_modules/boat_torch/utils/op_utils.html#DynamicalSystemRules"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#boat_torch.utils.op_utils.DynamicalSystemRules" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>A class to store and manage gradient operator rules.</p>
<dl class="py method">
<dt class="sig sig-object py" id="boat_torch.utils.op_utils.DynamicalSystemRules.get_gradient_order">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">get_gradient_order</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="_modules/boat_torch/utils/op_utils.html#DynamicalSystemRules.get_gradient_order"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#boat_torch.utils.op_utils.DynamicalSystemRules.get_gradient_order" title="Link to this definition"></a></dt>
<dd><p>Get the current gradient operator order.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The current gradient operator order.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>List[List[str]]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="boat_torch.utils.op_utils.DynamicalSystemRules.set_gradient_order">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">set_gradient_order</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">new_order</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/boat_torch/utils/op_utils.html#DynamicalSystemRules.set_gradient_order"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#boat_torch.utils.op_utils.DynamicalSystemRules.set_gradient_order" title="Link to this definition"></a></dt>
<dd><p>Set a new gradient operator order.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>new_order</strong> (<em>List</em><em>[</em><em>List</em><em>[</em><em>str</em><em>]</em><em>]</em>) – The new gradient operator order to set.</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>ValueError</strong> – If the new order is invalid.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="boat_torch.utils.op_utils.HyperGradientRules">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">boat_torch.utils.op_utils.</span></span><span class="sig-name descname"><span class="pre">HyperGradientRules</span></span><a class="reference internal" href="_modules/boat_torch/utils/op_utils.html#HyperGradientRules"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#boat_torch.utils.op_utils.HyperGradientRules" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>A class to store and manage gradient operator rules.</p>
<dl class="py method">
<dt class="sig sig-object py" id="boat_torch.utils.op_utils.HyperGradientRules.get_gradient_order">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">get_gradient_order</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="_modules/boat_torch/utils/op_utils.html#HyperGradientRules.get_gradient_order"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#boat_torch.utils.op_utils.HyperGradientRules.get_gradient_order" title="Link to this definition"></a></dt>
<dd><p>Get the current gradient operator order.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The current gradient operator order.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>List[List[str]]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="boat_torch.utils.op_utils.HyperGradientRules.set_gradient_order">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">set_gradient_order</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">new_order</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/boat_torch/utils/op_utils.html#HyperGradientRules.set_gradient_order"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#boat_torch.utils.op_utils.HyperGradientRules.set_gradient_order" title="Link to this definition"></a></dt>
<dd><p>Set a new gradient operator order.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>new_order</strong> (<em>List</em><em>[</em><em>List</em><em>[</em><em>str</em><em>]</em><em>]</em>) – The new gradient operator order to set.</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>ValueError</strong> – If the new order is invalid.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="boat_torch.utils.op_utils.ResultStore">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">boat_torch.utils.op_utils.</span></span><span class="sig-name descname"><span class="pre">ResultStore</span></span><a class="reference internal" href="_modules/boat_torch/utils/op_utils.html#ResultStore"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#boat_torch.utils.op_utils.ResultStore" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>A simple class to store and manage intermediate results of hyper-gradient computation.</p>
<dl class="py method">
<dt class="sig sig-object py" id="boat_torch.utils.op_utils.ResultStore.add">
<span class="sig-name descname"><span class="pre">add</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">result</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/boat_torch/utils/op_utils.html#ResultStore.add"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#boat_torch.utils.op_utils.ResultStore.add" title="Link to this definition"></a></dt>
<dd><p>Add a result to the store.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<em>str</em>) – The name of the result (e.g., ‘gradient_operator_results_0’).</p></li>
<li><p><strong>result</strong> (<em>Dict</em>) – The result dictionary to store.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="boat_torch.utils.op_utils.ResultStore.clear">
<span class="sig-name descname"><span class="pre">clear</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/boat_torch/utils/op_utils.html#ResultStore.clear"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#boat_torch.utils.op_utils.ResultStore.clear" title="Link to this definition"></a></dt>
<dd><p>Clear all stored results.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="boat_torch.utils.op_utils.ResultStore.get_results">
<span class="sig-name descname"><span class="pre">get_results</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="_modules/boat_torch/utils/op_utils.html#ResultStore.get_results"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#boat_torch.utils.op_utils.ResultStore.get_results" title="Link to this definition"></a></dt>
<dd><p>Retrieve all stored results.</p>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="boat_torch.utils.op_utils.average_grad">
<span class="sig-prename descclassname"><span class="pre">boat_torch.utils.op_utils.</span></span><span class="sig-name descname"><span class="pre">average_grad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/boat_torch/utils/op_utils.html#average_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#boat_torch.utils.op_utils.average_grad" title="Link to this definition"></a></dt>
<dd><p>Average gradients over a batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<em>torch.nn.Module</em>) – Model whose gradients are averaged.</p></li>
<li><p><strong>batch_size</strong> (<em>int</em>) – The batch size used for averaging.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="boat_torch.utils.op_utils.cat_list_to_tensor">
<span class="sig-prename descclassname"><span class="pre">boat_torch.utils.op_utils.</span></span><span class="sig-name descname"><span class="pre">cat_list_to_tensor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">list_tx</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/boat_torch/utils/op_utils.html#cat_list_to_tensor"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#boat_torch.utils.op_utils.cat_list_to_tensor" title="Link to this definition"></a></dt>
<dd><p>Concatenate a list of tensors into a single tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>list_tx</strong> (<em>List</em><em>[</em><em>Tensor</em><em>]</em>) – List of tensors to concatenate.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The concatenated tensor.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="boat_torch.utils.op_utils.cg_step">
<span class="sig-prename descclassname"><span class="pre">boat_torch.utils.op_utils.</span></span><span class="sig-name descname"><span class="pre">cg_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">Ax</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">b</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epsilon</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/boat_torch/utils/op_utils.html#cg_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#boat_torch.utils.op_utils.cg_step" title="Link to this definition"></a></dt>
<dd><p>Perform the conjugate gradient optimization step.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>Ax</strong> (<em>Callable</em>) – Function to compute the matrix-vector product.</p></li>
<li><p><strong>b</strong> (<em>List</em><em>[</em><em>Tensor</em><em>]</em>) – Right-hand side of the linear system.</p></li>
<li><p><strong>max_iter</strong> (<em>int</em><em>, </em><em>optional</em>) – Maximum number of iterations, by default 100.</p></li>
<li><p><strong>epsilon</strong> (<em>float</em><em>, </em><em>optional</em>) – Tolerance for early stopping, by default 1.0e-5.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Solution vector for the linear system.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>List[Tensor]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="boat_torch.utils.op_utils.conjugate_gradient">
<span class="sig-prename descclassname"><span class="pre">boat_torch.utils.op_utils.</span></span><span class="sig-name descname"><span class="pre">conjugate_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">params</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hparams</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">upper_loss</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lower_loss</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">K</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fp_map</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-10</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="_modules/boat_torch/utils/op_utils.html#conjugate_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#boat_torch.utils.op_utils.conjugate_gradient" title="Link to this definition"></a></dt>
<dd><p>Compute hypergradients using the conjugate gradient method.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> (<em>List</em><em>[</em><em>Tensor</em><em>]</em>) – List of lower-level parameters.</p></li>
<li><p><strong>hparams</strong> (<em>List</em><em>[</em><em>Tensor</em><em>]</em>) – List of upper-level hyperparameters.</p></li>
<li><p><strong>upper_loss</strong> (<em>Tensor</em>) – The upper-level loss.</p></li>
<li><p><strong>lower_loss</strong> (<em>Tensor</em>) – The lower-level loss.</p></li>
<li><p><strong>K</strong> (<em>int</em>) – Maximum number of iterations for the conjugate gradient method.</p></li>
<li><p><strong>fp_map</strong> (<em>Callable</em>) – Fixed-point mapping function.</p></li>
<li><p><strong>tol</strong> (<em>float</em><em>, </em><em>optional</em>) – Tolerance for early stopping, by default 1e-10.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Hypergradients for the upper-level hyperparameters.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>List[Tensor]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="boat_torch.utils.op_utils.copy_parameter_from_list">
<span class="sig-prename descclassname"><span class="pre">boat_torch.utils.op_utils.</span></span><span class="sig-name descname"><span class="pre">copy_parameter_from_list</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">z</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/boat_torch/utils/op_utils.html#copy_parameter_from_list"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#boat_torch.utils.op_utils.copy_parameter_from_list" title="Link to this definition"></a></dt>
<dd><p>Copy parameters from a list to a model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y</strong> (<em>torch.nn.Module</em>) – Target model to which parameters are copied.</p></li>
<li><p><strong>z</strong> (<em>List</em><em>[</em><em>torch.Tensor</em><em>]</em>) – List of source parameters.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The updated model with copied parameters.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.nn.Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="boat_torch.utils.op_utils.get_outer_gradients">
<span class="sig-prename descclassname"><span class="pre">boat_torch.utils.op_utils.</span></span><span class="sig-name descname"><span class="pre">get_outer_gradients</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outer_loss</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">params</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hparams</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retain_graph</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/boat_torch/utils/op_utils.html#get_outer_gradients"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#boat_torch.utils.op_utils.get_outer_gradients" title="Link to this definition"></a></dt>
<dd><p>Compute the gradients of the outer-level loss with respect to parameters and hyperparameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>outer_loss</strong> (<em>Tensor</em>) – The outer-level loss.</p></li>
<li><p><strong>params</strong> (<em>List</em><em>[</em><em>Tensor</em><em>]</em>) – List of tensors representing parameters.</p></li>
<li><p><strong>hparams</strong> (<em>List</em><em>[</em><em>Tensor</em><em>]</em>) – List of tensors representing hyperparameters.</p></li>
<li><p><strong>retain_graph</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to retain the computation graph, by default True.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Gradients with respect to parameters and hyperparameters.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Tuple[List[Tensor], List[Tensor]]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="boat_torch.utils.op_utils.grad_unused_zero">
<span class="sig-prename descclassname"><span class="pre">boat_torch.utils.op_utils.</span></span><span class="sig-name descname"><span class="pre">grad_unused_zero</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_outputs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retain_graph</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">create_graph</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/boat_torch/utils/op_utils.html#grad_unused_zero"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#boat_torch.utils.op_utils.grad_unused_zero" title="Link to this definition"></a></dt>
<dd><p>Compute gradients for the given inputs, substituting zeros for unused gradients.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output</strong> (<em>torch.Tensor</em>) – The output tensor for which gradients are computed.</p></li>
<li><p><strong>inputs</strong> (<em>List</em><em>[</em><em>torch.Tensor</em><em>]</em>) – List of input tensors with respect to which gradients are computed.</p></li>
<li><p><strong>grad_outputs</strong> (<em>torch.Tensor</em><em>, </em><em>optional</em>) – Gradient outputs to compute the gradients of the inputs, by default None.</p></li>
<li><p><strong>retain_graph</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, the computation graph is retained after the gradient computation,
by default False.</p></li>
<li><p><strong>create_graph</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, constructs the graph for higher-order gradient computations,
by default False.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Gradients for the inputs, with unused gradients replaced by zeros.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Tuple[torch.Tensor]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="boat_torch.utils.op_utils.l2_reg">
<span class="sig-prename descclassname"><span class="pre">boat_torch.utils.op_utils.</span></span><span class="sig-name descname"><span class="pre">l2_reg</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">parameters</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/boat_torch/utils/op_utils.html#l2_reg"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#boat_torch.utils.op_utils.l2_reg" title="Link to this definition"></a></dt>
<dd><p>Compute the L2 regularization term for a list of parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>parameters</strong> (<em>List</em><em>[</em><em>torch.Tensor</em><em>]</em>) – List of tensors for which the L2 regularization term is computed.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The L2 regularization loss value.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="boat_torch.utils.op_utils.list_tensor_matmul">
<span class="sig-prename descclassname"><span class="pre">boat_torch.utils.op_utils.</span></span><span class="sig-name descname"><span class="pre">list_tensor_matmul</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">list1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">list2</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/boat_torch/utils/op_utils.html#list_tensor_matmul"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#boat_torch.utils.op_utils.list_tensor_matmul" title="Link to this definition"></a></dt>
<dd><p>Perform element-wise multiplication and summation for two lists of tensors.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>list1</strong> (<em>List</em><em>[</em><em>torch.Tensor</em><em>]</em>) – First list of tensors.</p></li>
<li><p><strong>list2</strong> (<em>List</em><em>[</em><em>torch.Tensor</em><em>]</em>) – Second list of tensors.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Result of the element-wise multiplication and summation.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="boat_torch.utils.op_utils.list_tensor_norm">
<span class="sig-prename descclassname"><span class="pre">boat_torch.utils.op_utils.</span></span><span class="sig-name descname"><span class="pre">list_tensor_norm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">list_tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">p</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/boat_torch/utils/op_utils.html#list_tensor_norm"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#boat_torch.utils.op_utils.list_tensor_norm" title="Link to this definition"></a></dt>
<dd><p>Compute the p-norm of a list of tensors.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>list_tensor</strong> (<em>List</em><em>[</em><em>torch.Tensor</em><em>]</em>) – List of tensors for which the norm is computed.</p></li>
<li><p><strong>p</strong> (<em>int</em><em>, </em><em>optional</em>) – Order of the norm, by default 2.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The computed p-norm of the list of tensors.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="boat_torch.utils.op_utils.neumann">
<span class="sig-prename descclassname"><span class="pre">boat_torch.utils.op_utils.</span></span><span class="sig-name descname"><span class="pre">neumann</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">params</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hparams</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">upper_loss</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lower_loss</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fp_map</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-10</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="_modules/boat_torch/utils/op_utils.html#neumann"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#boat_torch.utils.op_utils.neumann" title="Link to this definition"></a></dt>
<dd><p>Compute hypergradients using Neumann series approximation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> (<em>List</em><em>[</em><em>Tensor</em><em>]</em>) – List of lower-level parameters.</p></li>
<li><p><strong>hparams</strong> (<em>List</em><em>[</em><em>Tensor</em><em>]</em>) – List of upper-level hyperparameters.</p></li>
<li><p><strong>upper_loss</strong> (<em>Tensor</em>) – The upper-level loss.</p></li>
<li><p><strong>lower_loss</strong> (<em>Tensor</em>) – The lower-level loss.</p></li>
<li><p><strong>k</strong> (<em>int</em>) – Number of iterations for Neumann approximation.</p></li>
<li><p><strong>fp_map</strong> (<em>Callable</em>) – Fixed-point mapping function.</p></li>
<li><p><strong>tol</strong> (<em>float</em><em>, </em><em>optional</em>) – Tolerance for early stopping, by default 1e-10.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Hypergradients for the upper-level hyperparameters.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>List[Tensor]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="boat_torch.utils.op_utils.require_model_grad">
<span class="sig-prename descclassname"><span class="pre">boat_torch.utils.op_utils.</span></span><span class="sig-name descname"><span class="pre">require_model_grad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/boat_torch/utils/op_utils.html#require_model_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#boat_torch.utils.op_utils.require_model_grad" title="Link to this definition"></a></dt>
<dd><p>Enable gradient computation for all parameters in the given model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>model</strong> (<em>torch.nn.Module</em><em>, </em><em>optional</em>) – The model whose parameters require gradient computation.</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>AssertionError</strong> – If the model is None.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="boat_torch.utils.op_utils.stop_grads">
<span class="sig-prename descclassname"><span class="pre">boat_torch.utils.op_utils.</span></span><span class="sig-name descname"><span class="pre">stop_grads</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">grads</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/boat_torch/utils/op_utils.html#stop_grads"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#boat_torch.utils.op_utils.stop_grads" title="Link to this definition"></a></dt>
<dd><p>Stop gradient computation for the given gradients.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>grads</strong> (<em>List</em><em>[</em><em>torch.Tensor</em><em>]</em>) – Gradients to be detached from the computation graph.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Gradients detached from the computation graph.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>List[torch.Tensor]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="boat_torch.utils.op_utils.stop_model_grad">
<span class="sig-prename descclassname"><span class="pre">boat_torch.utils.op_utils.</span></span><span class="sig-name descname"><span class="pre">stop_model_grad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/boat_torch/utils/op_utils.html#stop_model_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#boat_torch.utils.op_utils.stop_model_grad" title="Link to this definition"></a></dt>
<dd><p>Disable gradient computation for all parameters in the given model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>model</strong> (<em>torch.nn.Module</em><em>, </em><em>optional</em>) – The model whose parameters no longer require gradient computation.</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>AssertionError</strong> – If the model is None.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="boat_torch.utils.op_utils.update_grads">
<span class="sig-prename descclassname"><span class="pre">boat_torch.utils.op_utils.</span></span><span class="sig-name descname"><span class="pre">update_grads</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">grads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/boat_torch/utils/op_utils.html#update_grads"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#boat_torch.utils.op_utils.update_grads" title="Link to this definition"></a></dt>
<dd><p>Update gradients of a model with the given gradients.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>grads</strong> (<em>List</em><em>[</em><em>torch.Tensor</em><em>]</em>) – Gradients to be applied to the model’s parameters.</p></li>
<li><p><strong>model</strong> (<em>torch.nn.Module</em>) – Model whose gradients are updated.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="boat_torch.utils.op_utils.update_tensor_grads">
<span class="sig-prename descclassname"><span class="pre">boat_torch.utils.op_utils.</span></span><span class="sig-name descname"><span class="pre">update_tensor_grads</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hparams</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grads</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/boat_torch/utils/op_utils.html#update_tensor_grads"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#boat_torch.utils.op_utils.update_tensor_grads" title="Link to this definition"></a></dt>
<dd><p>Update gradients of hyperparameters with the given gradients.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hparams</strong> (<em>List</em><em>[</em><em>torch.Tensor</em><em>]</em>) – Hyperparameters whose gradients are updated.</p></li>
<li><p><strong>grads</strong> (<em>List</em><em>[</em><em>torch.Tensor</em><em>]</em>) – Gradients to be applied to the hyperparameters.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="boat_torch.hyper_ol.html" class="btn btn-neutral float-left" title="boat_torch.hyper_ol" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="data_hyper_cleaning_example.html" class="btn btn-neutral float-right" title="Data HyperCleaning" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Yaohua Liu.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>